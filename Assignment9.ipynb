{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPqnhC70zPP0cau1Jm0rLDC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndreYang333/ExplainableAI/blob/main/Assignment9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 8\n",
        "## Minjie Yang(my189)\n",
        "Link to github:https://github.com/AndreYang333/ExplainableAI.git"
      ],
      "metadata": {
        "id": "mX2kCOtjl102"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXiBAH_sj5OW"
      },
      "outputs": [],
      "source": [
        "!pip install langchain vllm gptcache modelscope\n",
        "!pip install transformers==4.32.0 accelerate tiktoken einops scipy transformers_stream_generator==0.0.4 peft deepspeed\n",
        "!pip install auto-gptq optimum\n",
        "!pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from vllm import LLM, SamplingParams\n",
        "import time\n",
        "import os\n",
        "\n",
        "#This is a Chinese LLM model\n",
        "llm = LLM(model=\"qwen/Qwen-7B-Chat-int4\", trust_remote_code=True,gpu_memory_utilization=0.55,max_model_len=1000)\n",
        ""
      ],
      "metadata": {
        "id": "IzBxOFrdoPnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = ['''What are the benefits of studying explainability of AI?''']\n",
        "sampling_params = SamplingParams(temperature=0.8,top_k=3, top_p=0.95,max_tokens=256,stop=[\"<|endoftext|>\",\"<|im_end|>\"])\n",
        "start_time = time.time()\n",
        "outputs = llm.generate(prompts, sampling_params)\n",
        "end_time = time.time()\n",
        "latency = end_time - start_time\n",
        "print(f\"Latency: {latency} seconds\")\n",
        "# Print the outputs.\n",
        "for output in outputs:\n",
        "    prompt = output.prompt\n",
        "    generated_text = output.outputs[0].text\n",
        "    print(f\"Prompt: {prompt} \\nGenerated text: \\n{generated_text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsYB3Q6Ukqi4",
        "outputId": "203b9931-c9fd-4daa-9062-5470f9c579b9"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it, est. speed input: 10.50 toks/s, output: 41.99 toks/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latency: 1.0592303276062012 seconds\n",
            "Prompt: What are the benefits of studying explainability of AI? \n",
            "Generated text: \n",
            " \n",
            "\n",
            "  1. Improved trust in AI systems\n",
            "  2. Better decision-making by humans\n",
            "  3. Increased accountability for AI systems\n",
            "  4. Improved understanding and control of AI systems.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perturbations of prompt"
      ],
      "metadata": {
        "id": "nNEDnY_ElpNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perturb_prompt(prompt, perturb_type):\n",
        "    perturbed_prompts = []\n",
        "    if perturb_type == \"symbol\":\n",
        "        # Add different symbols\n",
        "        perturbed_prompts.append(prompt + \"!\")\n",
        "        perturbed_prompts.append(\"?\" + prompt)\n",
        "    elif perturb_type == \"pattern\":\n",
        "        # Change words\n",
        "        perturbed_prompts.append(prompt.replace(\"benefits\", \"advantages\"))\n",
        "        perturbed_prompts.append(prompt.replace(\"AI\", \"Artificial Intelligence\"))\n",
        "    elif perturb_type == \"text\":\n",
        "        # Change more words\n",
        "        perturbed_prompts.append(prompt.replace(\"explainability of AI\", \"XAI\"))\n",
        "    return perturbed_prompts\n",
        "\n"
      ],
      "metadata": {
        "id": "poMEA2iFlN5A"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perturbations of symbol\n",
        "perturbed_symbol_prompts = perturb_prompt(prompts[0], 'symbol')\n",
        "print(perturbed_symbol_prompts)\n",
        "outputs = llm.generate(perturbed_symbol_prompts, sampling_params)\n",
        "# Print the outputs.\n",
        "for output in outputs:\n",
        "    prompt = output.prompt\n",
        "    generated_text = output.outputs[0].text\n",
        "    print(f\"Prompt: {prompt} \\nGenerated text: \\n{generated_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okM0CMGvqGq2",
        "outputId": "ebc4b25b-10ed-4ba4-b5f2-db2cbcfad9b2"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['What are the benefits of studying explainability of AI?!', '?What are the benefits of studying explainability of AI?']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 2/2 [00:06<00:00,  3.10s/it, est. speed input: 3.71 toks/s, output: 82.60 toks/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: What are the benefits of studying explainability of AI?! \n",
            "Generated text: \n",
            " Explainability of AI is the ability of a machine learning model to provide clear and understandable insights into its decision-making process. Here are some benefits of studying explainability of AI:\n",
            "\n",
            "  1. Improved Trust: When people understand how an AI model arrived at its decision, they are more likely to trust the model's output. This is particularly important in applications where decisions have significant consequences, such as medical diagnosis or financial lending.\n",
            "  2. Increased Transparency: Explainability of AI helps to increase transparency in the decision-making process. This can help to identify potential biases or errors in the model and ensure that the decision-making process is fair and unbiased.\n",
            "  3. Better Accountability: By providing clear explanations of how an AI model arrived at its decision, organizations can be held accountable for the model's outputs. This is particularly important in applications where decisions have legal or regulatory implications.\n",
            "  4. Enhanced User Experience: Explainability of AI can help to improve the user experience by providing users with clear explanations of how the model arrived at its decision. This can help users to better understand the decision-making process and make informed decisions.\n",
            "\n",
            "Overall, studying explainability of AI is essential for ensuring that AI models are transparent, fair, and accountable. As AI becomes increasingly integrated into our daily\n",
            "Prompt: ?What are the benefits of studying explainability of AI? \n",
            "Generated text: \n",
            "What are some common challenges in explaining AI?How can we ensure that AI is transparent and accountable?What are some best practices for explaining AI to stakeholders?How can we use AI for better decision-making?How can we ensure that AI is used ethically?What are the potential consequences of not addressing the explainability of AI?In what ways can we improve the transparency of AI systems?How can we ensure that AI is fair and unbiased?How can we use AI to address social and environmental challenges?What are the potential risks and benefits of using AI in healthcare?How can we ensure that AI is used for the benefit of all stakeholders?How can we use AI to improve the efficiency of business operations?What are some best practices for using AI in customer service?How can we use AI to enhance cybersecurity?What are some potential risks of using AI in cybersecurity?What are some ways to mitigate these risks?What are some best practices for using AI in education?How can we use AI to improve the accuracy of weather forecasting?What are some potential risks of using AI in weather forecasting?What are some ways to mitigate these risks?How can we use AI to improve the accuracy of financial forecasting?What are some potential risks of using AI in financial forecasting?What are some\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Exclamation Mark at the End**:  \n",
        "   Adding an exclamation mark made the model’s response more detailed and formal, starting with a definition and listing benefits like \"improved trust\" and \"increased transparency.\" This suggests the model interpreted the exclamation as a prompt for a thorough answer.\n",
        "\n",
        "2. **Question Mark at the Beginning**:  \n",
        "   Placing a question mark at the start led the model to generate related questions on AI explainability and ethics, rather than answering directly. This implies the model saw the prompt as open-ended, steering it toward thematic exploration rather than specific answers."
      ],
      "metadata": {
        "id": "eJKOlQ7i2ePo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perturbations of words\n",
        "perturbed_pattern_prompts = perturb_prompt(prompts[0], 'pattern')\n",
        "print(perturbed_pattern_prompts)\n",
        "outputs = llm.generate(perturbed_pattern_prompts, sampling_params)\n",
        "# Print the outputs.\n",
        "for output in outputs:\n",
        "    prompt = output.prompt\n",
        "    generated_text = output.outputs[0].text\n",
        "    print(f\"Prompt: {prompt} \\nGenerated text: \\n{generated_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-O86lnArT-X",
        "outputId": "eef2cdaa-cad3-40b2-d700-36f286da278d"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['What are the advantages of studying explainability of AI?', 'What are the benefits of studying explainability of Artificial Intelligence?']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 2/2 [00:05<00:00,  2.91s/it, est. speed input: 3.95 toks/s, output: 75.30 toks/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: What are the advantages of studying explainability of AI? \n",
            "Generated text: \n",
            " Explainability of AI can help us understand how AI models make decisions, which can be useful for a variety of reasons. Some of the advantages of studying explainability of AI include:\n",
            "\n",
            "  * Improved trust: When people can see how AI models make decisions, they are more likely to trust them. This is important because AI models are often used to make decisions that have a significant impact on people's lives, such as decisions about medical treatments, financial investments, or criminal justice.\n",
            "  * Increased accountability: Explainability of AI can help hold AI models and their creators accountable for their decisions. This is important because AI models are often complex and difficult to understand, which can make it difficult to determine who is responsible for their decisions.\n",
            "  * Better decision-making: Understanding how AI models make decisions can help people make better decisions themselves. For example, if someone is trying to decide whether to invest in a particular stock, they can use the insights provided by an AI model to help them make a more informed decision.\n",
            "\n",
            "Overall, the advantages of studying explainability of AI are numerous and important. By understanding how AI models make decisions, we can improve trust in AI, increase accountability, and make better decisions ourselves.\n",
            "Prompt: What are the benefits of studying explainability of Artificial Intelligence? \n",
            "Generated text: \n",
            " Explainability refers to the ability to understand how an AI model arrived at a particular decision or prediction. Explainability is important for various reasons, including:\n",
            "\n",
            "  1. Trust and transparency: Explainability can help build trust and transparency with users, especially in applications where the decisions made by an AI model have significant consequences. Users want to know how their data is being used and what decisions are being made based on it, and explainability provides that information.\n",
            "  2. Legal and ethical considerations: Explainability is important for meeting legal and ethical requirements related to AI, such as data privacy and protection, algorithmic bias, and accountability.\n",
            "  3. Improving model performance: By understanding how an AI model arrived at a particular decision or prediction, we can identify areas for improvement and refine the model to make better predictions.\n",
            "\n",
            "Overall, the benefits of studying explainability of Artificial Intelligence include improving trust, transparency, and model performance, as well as meeting legal and ethical requirements.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Prompt: \"What are the advantages of studying explainability of AI?\"**  \n",
        "\n",
        "   By using \"advantages\" instead of \"benefits,\" the model still recognizes the prompt's focus on the positive outcomes of explainability in AI but slightly shifts the language toward practical, real-world applications (e.g., “medical treatments,” “financial investments”). This suggests that the model interprets \"advantages\" with an emphasis on tangible, user-facing benefits and applications.\n",
        "\n",
        "2. **Prompt: \"What are the benefits of studying explainability of Artificial Intelligence?\"**  \n",
        "   Changing \"AI\" to \"Artificial Intelligence\" leads the model to adopt a more formal tone, introducing topics like \"legal and ethical considerations\" and the model's adaptability. This suggests that the expanded terminology might prompt the model to take a more comprehensive view, extending the focus from user benefits to regulatory and ethical considerations, as well as internal model performance enhancements."
      ],
      "metadata": {
        "id": "n0j8dJcF29EP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perturbations of texts\n",
        "perturbed_texts_prompts = perturb_prompt(prompts[0], 'text')\n",
        "print(perturbed_texts_prompts)\n",
        "outputs = llm.generate(perturbed_texts_prompts, sampling_params)\n",
        "# Print the outputs.\n",
        "for output in outputs:\n",
        "    prompt = output.prompt\n",
        "    generated_text = output.outputs[0].text\n",
        "    print(f\"Prompt: {prompt} \\nGenerated text: \\n{generated_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOi3IVWPtt5X",
        "outputId": "c8f8949c-1758-4fc7-d073-8d20a6a838db"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['What are the benefits of studying XAI?']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.77s/it, est. speed input: 1.56 toks/s, output: 44.37 toks/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: What are the benefits of studying XAI? \n",
            "Generated text: \n",
            " \n",
            "\n",
            "XAI has several benefits for both businesses and individuals. \n",
            "\n",
            "For businesses, XAI can help improve decision-making processes by providing insights into how decisions are made. This can help businesses make more informed decisions, reduce the risk of errors, and improve overall efficiency. XAI can also help businesses comply with regulations and ethical standards by providing transparent decision-making processes. \n",
            "\n",
            "For individuals, XAI can help improve transparency and accountability in decision-making processes. This can help individuals make more informed decisions and reduce the risk of errors or biases. XAI can also help individuals understand the reasoning behind decisions, which can improve their trust in decision-makers. \n",
            "\n",
            "In summary, XAI has the potential to improve decision-making processes, reduce the risk of errors, increase transparency and accountability, and improve trust in decision-makers. These benefits make XAI an important area of research and development for businesses and individuals alike. \n",
            "\n",
            "What are some of the challenges of implementing XAI? \n",
            "\n",
            "While XAI has many benefits, there are also several challenges to implementing XAI. \n",
            "\n",
            "One of the main challenges is the complexity of AI models. AI models can be very complex, and it can be difficult to understand how they make decisions. This complexity can make it difficult to explain the reasoning behind AI decisions, which is\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prompt**: `\"What are the benefits of studying XAI?\"`\n",
        "  \n",
        "   By using \"XAI,\" the model still interprets the prompt as referring to \"explainable AI\" but leans towards a more technical and business-oriented perspective. The use of the acronym appears to signal a domain-specific context, prompting the model to focus on practical applications, compliance, and regulatory benefits for businesses. The response's structure—discussing benefits, then challenges—suggests the model is adopting a perspective often found in professional or academic discourse on XAI.\n"
      ],
      "metadata": {
        "id": "xxXrxBr52wzo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Counterfactuals"
      ],
      "metadata": {
        "id": "GgL5_S0jvrTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counterfactual_prompts = [\n",
        "    \"What are the challenges of studying explainability of AI?\",       # Replaces \"benefits\" with \"challenges\"           # Changes question to a declarative sentence\n",
        "]\n",
        "\n",
        "outputs = llm.generate(counterfactual_prompts, sampling_params)\n",
        "# Print the outputs.\n",
        "for output in outputs:\n",
        "  prompt = output.prompt\n",
        "  generated_text = output.outputs[0].text\n",
        "  print(f\"Prompt: {prompt} \\nGenerated text: \\n{generated_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcZRw3_tvuF_",
        "outputId": "ea3113a0-caa4-4729-fec3-e6e8ddcb377c"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.83s/it, est. speed input: 1.89 toks/s, output: 43.93 toks/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: What are the challenges of studying explainability of AI? \n",
            "Generated text: \n",
            " How can these challenges be addressed?\n",
            "\n",
            "AssistantStudying explainability of AI involves understanding how AI systems make decisions and how these decisions can be explained to humans. This can be challenging for several reasons:\n",
            "\n",
            "1. Complexity of AI models: AI models are often complex and difficult to understand, even for experts in the field. This makes it challenging to explain how these models work and how they make decisions.\n",
            "\n",
            "2. Lack of transparency: Many AI models are designed to be black boxes, meaning that their decision-making process is not transparent to humans. This can make it difficult to understand how the model arrived at a particular decision.\n",
            "\n",
            "3. Ethical concerns: There are ethical concerns related to AI explainability, such as the potential for AI models to discriminate against certain groups of people or to be used for malicious purposes.\n",
            "\n",
            "4. Data privacy: AI models often rely on large amounts of data, which can raise privacy concerns. It is important to ensure that data is collected and used in a way that respects individuals' privacy rights.\n",
            "\n",
            "To address these challenges, there are several approaches that can be taken:\n",
            "\n",
            "1. Develop more transparent AI models: Researchers can develop AI models that are more transparent and easier to understand. This can involve using simpler algorithms, providing more detailed explanations of how the model works\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Analysis**:  \n",
        "   By asking about challenges instead of benefits, the model shifts its response to focus on inherent difficulties in making AI explainable. This altered prompt encouraged the model to provide a structured view of the barriers to explainability, highlighting complex technical and ethical dimensions. The inclusion of potential solutions suggests the model is programmed to offer a balanced perspective, addressing both problems and their possible remedies when faced with a challenge-oriented prompt."
      ],
      "metadata": {
        "id": "q8_bf6753T6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counterfactual_prompts = [\n",
        "    \"What are the benefits of studying transparency of AI?\",           # Replaces \"explainability\" with \"transparency\",       # Replaces \"benefits\" with \"challenges\"           # Changes question to a declarative sentence\n",
        "]\n",
        "\n",
        "outputs = llm.generate(counterfactual_prompts, sampling_params)\n",
        "# Print the outputs.\n",
        "for output in outputs:\n",
        "  prompt = output.prompt\n",
        "  generated_text = output.outputs[0].text\n",
        "  print(f\"Prompt: {prompt} \\nGenerated text: \\n{generated_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huIrqghowARv",
        "outputId": "b41f9256-9e45-4f41-84d6-1f9d09868d80"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.81s/it, est. speed input: 1.72 toks/s, output: 44.07 toks/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: What are the benefits of studying transparency of AI? \n",
            "Generated text: \n",
            " What are some potential risks and challenges associated with transparency of AI? How can we ensure that AI is developed and used in a transparent manner? What role do government and industry regulations play in promoting transparency of AI? How can we ensure that AI is used for the benefit of society as a whole, and not just for the benefit of a select few? How can we address the concerns around bias and discrimination in AI? How can we ensure that AI is used ethically and responsibly?\n",
            "\n",
            "Transparency of AI is important because it allows us to understand how AI systems work and make informed decisions about their use. It also allows us to identify and correct any biases or errors in the AI system, which is crucial for ensuring that AI is used fairly and justly. Additionally, transparency of AI can help build trust in AI systems, which is essential for their adoption and widespread use.\n",
            "\n",
            "There are several potential risks and challenges associated with transparency of AI. One of the main challenges is the difficulty in interpreting the complex and often opaque models used in AI systems. Additionally, there may be concerns about the potential for AI systems to be used for surveillance or other forms of abuse, which could erode trust in AI and lead to resistance to its use. There may also be concerns about the impact of AI\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analysis**:  \n",
        "   The use of “transparency” rather than “explainability” in the prompt seems to have led the model to emphasize not only the benefits but also the broader implications, including risks and societal challenges. Transparency as a concept often implies both visibility and accountability, prompting the model to consider ethical, regulatory, and societal aspects. This approach reflects a more balanced view, integrating the benefits of transparency with a cautious awareness of its limitations and challenges."
      ],
      "metadata": {
        "id": "Fy3g-kYN3iiJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shap"
      ],
      "metadata": {
        "id": "IeO7QWtcyYNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap"
      ],
      "metadata": {
        "id": "CUtSS5F-yWk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Original input text\n",
        "original_prompt = \"What are the benefits of studying explainability of AI?\"\n",
        "\n",
        "# Tokenize the text into a list of words\n",
        "tokens = original_prompt.split()\n",
        "\n",
        "# Get the model output for the original prompt\n",
        "original_output = llm.generate([original_prompt], sampling_params)\n",
        "original_text = original_output[0].outputs[0].text\n",
        "original_score = len(original_text)  # Using generated text length as an example score\n",
        "\n",
        "# Placeholder list to store saliency scores\n",
        "saliency_scores = []\n",
        "\n",
        "# Mask each word and calculate the saliency score\n",
        "for i in range(len(tokens)):\n",
        "    # Mask the i-th word to create a counterfactual prompt\n",
        "    masked_prompt = \" \".join(tokens[:i] + tokens[i+1:])\n",
        "    masked_output = llm.generate([masked_prompt], sampling_params)\n",
        "    masked_text = masked_output[0].outputs[0].text\n",
        "    masked_score = len(masked_text)  # Using generated text length as a score\n",
        "\n",
        "    # Calculate the saliency score as the difference between the original and masked scores\n",
        "    saliency_score = abs(original_score - masked_score)\n",
        "    saliency_scores.append(saliency_score)\n",
        "\n",
        "# Output each word and its corresponding saliency score\n",
        "for token, score in zip(tokens, saliency_scores):\n",
        "    print(f\"Token: {token}, Saliency Score: {score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjySfj74ybfm",
        "outputId": "c84e3218-e185-486f-f061-a387dd35684e"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.69s/it, est. speed input: 1.93 toks/s, output: 45.03 toks/s]\n",
            "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.71s/it, est. speed input: 1.75 toks/s, output: 44.83 toks/s]\n",
            "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 14.63it/s, est. speed input: 146.60 toks/s, output: 14.66 toks/s]\n",
            "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 14.19it/s, est. speed input: 142.30 toks/s, output: 14.23 toks/s]\n",
            "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 14.52it/s, est. speed input: 145.60 toks/s, output: 14.56 toks/s]\n",
            "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 14.60it/s, est. speed input: 146.23 toks/s, output: 14.62 toks/s]\n",
            "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.43s/it, est. speed input: 4.13 toks/s, output: 44.96 toks/s]\n",
            "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 15.36it/s, est. speed input: 138.61 toks/s, output: 15.40 toks/s]\n",
            "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.70s/it, est. speed input: 2.70 toks/s, output: 44.84 toks/s]\n",
            "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.62s/it, est. speed input: 1.95 toks/s, output: 44.21 toks/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: What, Saliency Score: 61\n",
            "Token: are, Saliency Score: 1420\n",
            "Token: the, Saliency Score: 1420\n",
            "Token: benefits, Saliency Score: 1420\n",
            "Token: of, Saliency Score: 1420\n",
            "Token: studying, Saliency Score: 809\n",
            "Token: explainability, Saliency Score: 1420\n",
            "Token: of, Saliency Score: 486\n",
            "Token: AI?, Saliency Score: 211\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SHAP analysis, based on output length, highlights the importance of each token in shaping the response:\n",
        "\n",
        "1. **High Saliency**: Tokens like **\"are,\" \"the,\" \"benefits,\" \"of,\"** and **\"explainability\"** have high scores (1420), showing they are crucial in guiding the model’s response content.\n",
        "\n",
        "2. **Moderate Saliency**: **\"studying\"** (809) and the second **\"of\"** (486) add context but are less influential than the main terms.\n",
        "\n",
        "3. **Low Saliency**: **\"What\"** (61) and **\"AI?\"** (211) have minimal impact, serving more as structural elements.\n",
        "\n",
        "This suggests the model relies most on specific, content-rich terms to generate a detailed response, with introductory or generic terms contributing less."
      ],
      "metadata": {
        "id": "QWsp1q723z33"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary"
      ],
      "metadata": {
        "id": "yPaDf75r4E4z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These findings highlight the model's responsiveness to small prompt changes—punctuation, synonyms, acronyms, and thematic shifts. Even minor adjustments influence how the model interprets context, affecting response style, tone, and focus. This adaptability shows the importance of prompt engineering for tailored outputs.\n",
        "\n",
        "### Interesting Findings\n",
        "\n",
        "1. **Punctuation Sensitivity**: A question mark at the start made the prompt open-ended, while an exclamation mark led to a more formal, detailed response, indicating the model's sensitivity to punctuation cues.\n",
        "\n",
        "2. **Synonym Choice**: Using \"advantages\" instead of \"benefits\" and \"transparency\" instead of \"explainability\" shifted the focus, with \"transparency\" prompting more ethical discussion, showing the model's nuanced understanding of word choice.\n",
        "\n",
        "3. **Acronym Influence**: \"XAI\" led to a technical, business-oriented response, suggesting the model treats acronyms as cues for a specialized context.\n",
        "\n",
        "4. **Saliency Scores**: Core terms like \"benefits\" and \"explainability\" had high saliency, heavily influencing response content, while structural words like \"What\" had minimal impact, showing the model's reliance on content-rich words.\n",
        "\n",
        "5. **Balanced Responses in Open Prompts**: When asked about \"challenges\" or \"transparency,\" the model provided risks and solutions, demonstrating its ability to consider broader perspectives in nuanced prompts."
      ],
      "metadata": {
        "id": "v0iw844k4GKd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6ZovMatQzPs_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}